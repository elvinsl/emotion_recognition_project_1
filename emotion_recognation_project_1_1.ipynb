{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognation Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 788 images belonging to 7 classes.\n",
      "Found 193 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory(directory='train/',\n",
    "                                              target_size=(48, 48),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory(directory='test/',\n",
    "                                            target_size=(48, 48),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=[48, 48, 3]))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 3s 115ms/step - loss: 1.8688 - accuracy: 0.2741 - val_loss: 1.7652 - val_accuracy: 0.4352\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 3s 107ms/step - loss: 1.6947 - accuracy: 0.4010 - val_loss: 1.4828 - val_accuracy: 0.5648\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 3s 111ms/step - loss: 1.2144 - accuracy: 0.5926 - val_loss: 0.9380 - val_accuracy: 0.7047\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.8144 - accuracy: 0.7132 - val_loss: 0.7011 - val_accuracy: 0.7617\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.5881 - accuracy: 0.7919 - val_loss: 0.5978 - val_accuracy: 0.8083\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 3s 111ms/step - loss: 0.4087 - accuracy: 0.8706 - val_loss: 0.6117 - val_accuracy: 0.8083\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 3s 119ms/step - loss: 0.4225 - accuracy: 0.8503 - val_loss: 0.5508 - val_accuracy: 0.8290\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 0.2969 - accuracy: 0.8845 - val_loss: 0.5530 - val_accuracy: 0.8135\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 3s 115ms/step - loss: 0.2708 - accuracy: 0.9048 - val_loss: 0.5776 - val_accuracy: 0.8083\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 3s 110ms/step - loss: 0.2259 - accuracy: 0.9251 - val_loss: 0.5569 - val_accuracy: 0.8031\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 3s 122ms/step - loss: 0.2289 - accuracy: 0.9226 - val_loss: 0.5529 - val_accuracy: 0.8497\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 3s 123ms/step - loss: 0.2095 - accuracy: 0.9277 - val_loss: 0.5694 - val_accuracy: 0.8187\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.1767 - accuracy: 0.9429 - val_loss: 0.6152 - val_accuracy: 0.8394\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.1404 - accuracy: 0.9569 - val_loss: 0.5576 - val_accuracy: 0.8705\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 3s 125ms/step - loss: 0.1588 - accuracy: 0.9530 - val_loss: 0.6350 - val_accuracy: 0.8497\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.1129 - accuracy: 0.9607 - val_loss: 0.6229 - val_accuracy: 0.8601\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 3s 122ms/step - loss: 0.1642 - accuracy: 0.9454 - val_loss: 0.5684 - val_accuracy: 0.8342\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 3s 124ms/step - loss: 0.1646 - accuracy: 0.9404 - val_loss: 0.6356 - val_accuracy: 0.8446\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 3s 123ms/step - loss: 0.1224 - accuracy: 0.9619 - val_loss: 0.5675 - val_accuracy: 0.8705\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.0901 - accuracy: 0.9708 - val_loss: 0.8419 - val_accuracy: 0.8187\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 3s 124ms/step - loss: 0.1076 - accuracy: 0.9581 - val_loss: 0.6114 - val_accuracy: 0.8497\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0733 - accuracy: 0.9746 - val_loss: 0.7628 - val_accuracy: 0.8497\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 3s 125ms/step - loss: 0.0889 - accuracy: 0.9683 - val_loss: 0.6456 - val_accuracy: 0.8497\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0949 - accuracy: 0.9695 - val_loss: 0.6103 - val_accuracy: 0.8601\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.1066 - accuracy: 0.9619 - val_loss: 0.5426 - val_accuracy: 0.8808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x229e2a73f48>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_set, validation_data=test_set, batch_size=32, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'contempt': 1,\n",
       " 'disgust': 2,\n",
       " 'fear': 3,\n",
       " 'happy': 4,\n",
       " 'sadness': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = image.load_img('test_image_1.jpg', target_size=(48, 48))\n",
    "test_img = image.img_to_array(test_img)\n",
    "test_img = np.expand_dims(test_img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Elvin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From D:\\Users\\Elvin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: saved_model/model1_88\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/model1_88')\n",
    "model.save('saved_model/model1_88.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
